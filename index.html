<!DOCTYPE html>
<html>
<head lang="en">
  <meta name="keywords" content="PointingFrameEstimator">
  <title>Pointing Frame Estimation with Audio-Visual Time Series Data for Daily Life Service Robots</title>
  <style>
    .container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 70px;
      flex-direction: column;
      text-align: center;
    }

    .fixed-width {
      width: 700px;
      margin: 10px auto;
    }

    .center-align {
      text-align: center;
    }

    .text-justify {
      text-align: justify;
    }

    video {
      width: 700px;
      height: auto;
      display: block;
      margin: 10px auto;
    }

    .col-md-8 {
      width: 700px;
      margin: 10px auto;
    }

    .github-link {
      display: flex;
      justify-content: center;
    }

    .github-link a {
      margin: 10px;
    }

    .image-container {
      display: flex;
      justify-content: center;
    }

    .image-container img {
      width: 700px;
      height: auto;
    }

    .video-container {
        margin: 10px;
        display: flex;
        justify-content: center;
    }
    
    .video-container iframe {
        width: 700px;
        height: 394px;
    }
  </style>
</head>
<body>
  <div class="container" id="main">
    <div class="row">
      <h2 class="col-md-12">
         Pointing Frame Estimation with Audio-Visual Time Series Data for Daily Life Service Robots
      </h2>
    </div>
  </div>
  
  <p class="center-align">Hikaru Nakagawa, <a href="https://scholar.google.com/citations?user=KPxSCJUAAAAJ&hl" target=“_blank” rel=“noopener noreferrer”>Shoichi Hasegawa*</a>, <a href="https://scholar.google.com/citations?user=Y4qjYvMAAAAJ&hl" target=“_blank” rel=“noopener noreferrer”>Yoshinobu Hagiwara</a>, <a href="https://scholar.google.com/citations?user=jtB7J0AAAAAJ&hl" target=“_blank” rel=“noopener noreferrer”>Akira Taniguchi</a>, <a href="https://scholar.google.com/citations?user=dPOCLQEAAAAJ&hl" target=“_blank” rel=“noopener noreferrer”>Tadahiro Taniguchi</a></p>
  
  <div class="github-link">
    <a href="https://github.com/EmergentSystemLabStudent/PointingFrameEstimator/tree/master" target=“_blank” rel=“noopener noreferrer”>Github</a>
    <!-- <a href="" target=“_blank” rel=“noopener noreferrer”>Paper</a>
    <a href="" target=“_blank” rel=“noopener noreferrer”>Slide</a> -->
  </div>


  <!-- <div class="video-container">
      <iframe src="https://www.youtube.com/embed/n8se-MgPi50?si=B5B60-8XXMO-j2CC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
  </div> -->

  <div class="image-container">
    <img src="./abstract_v3.svg" alt="Overview of abstract">
    <div class="image-caption">Figure 1: Overview of abstract</div>
  </div>
  
  <h2 class="fixed-width">Abstract</h2>
  <p class="center-align fixed-width text-justify">
    Daily life support robots in the home environment interpret the user’s pointing and understand the instructions, thereby increasing the number of instructions accomplished.
    This study aims to improve the estimation performance of pointing frames by using speech information when a person gives pointing or verbal instructions to the robot.
    The estimation of the pointing frame, which represents the moment when the user points, can help the user understand the instructions. 
    Therefore, we perform pointing frame estimation using a time-series model, utilizing the user’s speech, images, and speech-recognized text observed by the robot. 
    In our experiments, we set up realistic communication conditions, such as speech containing everyday conversation, non-upright posture, actions other than pointing, and reference objects outside the robot’s field of view. 
    The results showed that adding speech information improved the estimation performance, especially the Transformer model with Mel-Spectrogram as a feature. 
    This study will lead to be applied to object localization and action planning in 3D environments by robots in the future. The
  </p>

  <h2 class="fixed-width">Approach</h2>
  <p class="center-align fixed-width text-justify">
  The purpose of our study is to verify the extent to which the performance of pointing frame estimations can be enhanced through the integration of human speech data during human-robot interactions involving gestures and language. 
  We introduce a pointing frame estimator that leverages human speech information across various scenarios where individuals provide instructions to a robot using gestures and language. 
　
  <h2>Conditions</h2>
    <ol>
        <li>A scenario involving verbal utterances including daily conversations</li>
        <li>A scenario in which a user's posture is not consistently upright</li>
        <li>A scenario in which a user's movements include actions beyond pointing</li>
      　<li>A scenario in which the referenced object is outside the visual scope of the robot</li>
    </ol>

  We gather audio-visual time series data from the robot under these specified conditions and train our pointing frame estimator, allowing us to accurately estimate pointing frames in real-world scenarios.
  We also investigate which audio features are better at estimating pointing frames.
  We incorporate speech data into the pointing frame estimation framework proposed by Chen et al. to estimate the pointing frame under conditions of natural communication between a human and a daily life service robot. 
  An overview of the proposed model is depicted in Fig.2.
  </p>

  <div class="image-container">
    <img src="./model_v3.svg" alt="Overview of our model">
    <div class="image-caption">Figure 2: Overview of our model</div>
  </div>

  <h2 class="fixed-width">Speech and Video Prosessing</h2>
  <p class="center-align fixed-width text-justify">
  </p>

  <h2 class="fixed-width">Features Fusion Module</h2>
  <p class="center-align fixed-width text-justify">
  </p>

  <h2 class="fixed-width">Object Rectangle Estimator</h2>
  <p class="center-align fixed-width text-justify">
  </p>

  <h2 class="fixed-width">Pointing Frame Estimator</h2>
  <p class="center-align fixed-width text-justify">
  </p>
  
    
  <div class="col-md-8">
    <h2 class="text-center">Acknowledgements</h2>
    <p class="text-justify">
      This work was supported by JSPS KAKENHI Grants-in-Aid for Scientific Research (Grant Numbers JP23K16975, 22K12212) JST Moonshot Research & Development Program (Grant Number JPMJMS2011).
    </p>
  </div>
  
  <script src="script.js"></script>
</body>
</html>
