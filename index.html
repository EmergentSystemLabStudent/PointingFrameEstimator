<!DOCTYPE html>
<html>
<head lang="en">
  <meta name="keywords" content="PointingFrameEstimator">
  <title>Pointing Frame Estimation with Audio-Visual Time Series Data for Daily Life Service Robots</title>
  <style>
    .container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 70px;
      flex-direction: column;
      text-align: center;
    }

    .fixed-width {
      width: 700px;
      margin: 10px auto;
    }

    .center-align {
      text-align: center;
    }

    .text-justify {
      text-align: justify;
    }

    video {
      width: 700px;
      height: auto;
      display: block;
      margin: 10px auto;
    }

    .col-md-8 {
      width: 700px;
      margin: 10px auto;
    }

    .github-link {
      display: flex;
      justify-content: center;
    }

    .github-link a {
      margin: 10px;
    }

    .image-container {
      display: flex;
      justify-content: center;
    }

    .image-container img {
      width: 700px;
      height: auto;
    }

    .video-container {
        margin: 10px;
        display: flex;
        justify-content: center;
    }
    
    .video-container iframe {
        width: 700px;
        height: 394px;
    }
  </style>
</head>
<body>
  <div class="container" id="main">
    <div class="row">
      <h2 class="col-md-12">
         Pointing Frame Estimation with Audio-Visual Time Series Data for Daily Life Service Robots
      </h2>
    </div>
  </div>
  
  <p class="center-align">Hikaru Nakagawa, <a href="https://scholar.google.com/citations?hl=en&user=jtB7J0AAAAAJ" target=“_blank” rel=“noopener noreferrer”>Shoichi Hasegawa*</a>, <a href="https://scholar.google.com/citations?hl=en&user=Y4qjYvMAAAAJ" target=“_blank” rel=“noopener noreferrer”>Yoshinobu Hagiwara</a>, <a href="https://scholar.google.co.jp/citations?user=tsm7qaQAAAAJ&hl" target=“_blank” rel=“noopener noreferrer”>Akira Taniguchi</a>, <a href="https://scholar.google.co.jp/citations?user=KPxSCJUAAAAJ&hl" target=“_blank” rel=“noopener noreferrer”>Tadahiro Taniguchi</a></p>
  
  <div class="github-link">
    <a href="https://github.com/EmergentSystemLabStudent/PointingFrameEstimator/tree/master" target=“_blank” rel=“noopener noreferrer”>Github</a>
    <!-- <a href="" target=“_blank” rel=“noopener noreferrer”>Paper</a>
    <a href="" target=“_blank” rel=“noopener noreferrer”>Slide</a> -->
  </div>

  <div class="video-container">
      <iframe src="https://www.youtube.com/embed/vv9k_wPkfIs?si=8zLy3EsD6SoPXLBy" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
  </div>
  
  <!-- <div class="video-container">
      <iframe src="https://www.youtube.com/embed/n8se-MgPi50?si=B5B60-8XXMO-j2CC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
  </div> -->
  
  <h2 class="fixed-width">Abstract</h2>
  <p class="center-align fixed-width text-justify">
    Daily life support robots in the home environment interpret the user’s pointing and understand the instructions, thereby increasing the number of instructions accomplished.
    This study aims to improve the estimation performance of pointing frames by using speech information when a person gives pointing or verbal instructions to the robot.
    The estimation of the pointing frame, which represents the moment when the user points, can help the user understand the instructions. 
    Therefore, we perform pointing frame estimation using a time-series model, utilizing the user’s speech, images, and speech-recognized text observed by the robot. 
    In our experiments, we set up realistic communication conditions, such as speech containing everyday conversation, non-upright posture, actions other than pointing, and reference objects outside the robot’s field of view. 
    The results showed that adding speech information improved the estimation performance, especially the Transformer model with Mel-Spectrogram as a feature. 
    This study will lead to be applied to object localization and action planning in 3D environments by robots in the future. The
  </p>

  <h2 class="fixed-width">Approach</h2>
  <p class="center-align fixed-width text-justify">
    
  </p>
  


  <div class="col-md-8">
    <h2 class="text-center">Acknowledgements</h2>
    <p class="text-justify">
      This work was supported by JSPS KAKENHI Grants-in-Aid for Scientific Research (Grant Numbers JP23K16975, 22K12212) JST Moonshot Research & Development Program (Grant Number JPMJMS2011).
    </p>
  </div>
  
  <script src="script.js"></script>
</body>
</html>
